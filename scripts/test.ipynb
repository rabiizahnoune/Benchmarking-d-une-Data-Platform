{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: option 'a' unknown.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dbgen: invalid option -- 'o'\n",
      "TPC-H Population Generator (Version 2.14.0 build 0)\n",
      "Copyright Transaction Processing Performance Council 1994 - 2010\n",
      "USAGE:\n",
      "dbgen [-{vf}][-T {pcsoPSOL}]\n",
      "\t[-s <scale>][-C <procs>][-S <step>]\n",
      "dbgen [-v] [-O m] [-s <scale>] [-U <updates>]\n",
      "\n",
      "Basic Options\n",
      "===========================\n",
      "-C <n> -- separate data set into <n> chunks (requires -S, default: 1)\n",
      "-f     -- force. Overwrite existing files\n",
      "-h     -- display this message\n",
      "-q     -- enable QUIET mode\n",
      "-s <n> -- set Scale Factor (SF) to  <n> (default: 1) \n",
      "-S <n> -- build the <n>th step of the data/update set (used with -C or -U)\n",
      "-U <n> -- generate <n> update sets\n",
      "-v     -- enable VERBOSE mode\n",
      "\n",
      "Advanced Options\n",
      "===========================\n",
      "-b <s> -- load distributions for <s> (default: dists.dss)\n",
      "-d <n> -- split deletes between <n> files (requires -U)\n",
      "-i <n> -- split inserts between <n> files (requires -U)\n",
      "-T c   -- generate cutomers ONLY\n",
      "-T l   -- generate nation/region ONLY\n",
      "-T L   -- generate lineitem ONLY\n",
      "-T n   -- generate nation ONLY\n",
      "-T o   -- generate orders/lineitem ONLY\n",
      "-T O   -- generate orders ONLY\n",
      "-T p   -- generate parts/partsupp ONLY\n",
      "-T P   -- generate parts ONLY\n",
      "-T r   -- generate region ONLY\n",
      "-T s   -- generate suppliers ONLY\n",
      "-T S   -- generate partsupp ONLY\n",
      "\n",
      "To generate the SF=1 (1GB), validation database population, use:\n",
      "\tdbgen -vf -s 1\n",
      "\n",
      "To generate updates for a SF=1 (1GB), use:\n",
      "\tdbgen -v -U 1 -s 1\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['dbgen', '-s', '10', '-f', '-o', '/app/data/tpch_raw/sf10']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Générer SF10 (10 Go)\u001b[39;00m\n\u001b[32m     26\u001b[39m sf10_dir = \u001b[33m\"\u001b[39m\u001b[33m/app/data/tpch_raw/sf10\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mgenerate_tpch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msf10_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Charger les données brutes (exemple : lineitem.tbl)\u001b[39;00m\n\u001b[32m     30\u001b[39m df = spark.read.option(\u001b[33m\"\u001b[39m\u001b[33mdelimiter\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m|\u001b[39m\u001b[33m\"\u001b[39m).csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msf10_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/lineitem.tbl\u001b[39m\u001b[33m\"\u001b[39m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mgenerate_tpch_data\u001b[39m\u001b[34m(scale_factor, output_dir)\u001b[39m\n\u001b[32m     20\u001b[39m os.makedirs(output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     21\u001b[39m cmd = [\u001b[33m\"\u001b[39m\u001b[33mdbgen\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m-s\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(scale_factor), \u001b[33m\"\u001b[39m\u001b[33m-f\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m-o\u001b[39m\u001b[33m\"\u001b[39m, output_dir]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDonnées TPC-H générées pour SF\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dans \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['dbgen', '-s', '10', '-f', '-o', '/app/data/tpch_raw/sf10']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialiser SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Generate TPC-H 10GB\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars\", \"/opt/bitnami/spark/jars/iceberg-spark-runtime-3.4_2.12-1.4.3.jar\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/app/data/iceberg\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Générer les données TPC-H\n",
    "def generate_tpch_data(scale_factor, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cmd = [\"dbgen\", \"-s\", str(scale_factor), \"-f\", \"-o\", output_dir]\n",
    "    subprocess.run(cmd, check=True)\n",
    "    print(f\"Données TPC-H générées pour SF{scale_factor} dans {output_dir}\")\n",
    "\n",
    "# Générer SF10 (10 Go)\n",
    "sf10_dir = \"/app/data/tpch_raw/sf10\"\n",
    "generate_tpch_data(10, sf10_dir)\n",
    "\n",
    "# Charger les données brutes (exemple : lineitem.tbl)\n",
    "df = spark.read.option(\"delimiter\", \"|\").csv(f\"{sf10_dir}/lineitem.tbl\", inferSchema=True)\n",
    "df = df.toDF(\"l_orderkey\", \"l_partkey\", \"l_suppkey\", \"l_linenumber\", \"l_quantity\", \n",
    "             \"l_extendedprice\", \"l_discount\", \"l_tax\", \"l_returnflag\", \"l_linestatus\", \n",
    "             \"l_shipdate\", \"l_commitdate\", \"l_receiptdate\", \"l_shipinstruct\", \n",
    "             \"l_shipmode\", \"l_comment\")\n",
    "\n",
    "# Cacher le DataFrame pour optimiser les écritures multiples\n",
    "df.cache()\n",
    "\n",
    "# Partitionner les données\n",
    "sf_configs = [\n",
    "    (\"sf1\", 0.1, 1),   # 10% = 1 Go\n",
    "    (\"sf5\", 0.5, 5),   # 50% = 5 Go\n",
    "    (\"sf10\", 1.0, 10)  # 100% = 10 Go\n",
    "]\n",
    "\n",
    "for sf_name, fraction, size_gb in sf_configs:\n",
    "    sampled_df = df.sample(fraction=fraction) if fraction < 1.0 else df\n",
    "\n",
    "    parquet_dir = f\"/app/data/parquet/{sf_name}\"\n",
    "    orc_dir = f\"/app/data/orc/{sf_name}\"\n",
    "    iceberg_dir = f\"/app/data/iceberg/{sf_name}\"\n",
    "    delta_dir = f\"/app/data/delta/{sf_name}\"\n",
    "\n",
    "    sampled_df.write.mode(\"overwrite\").format(\"parquet\").partitionBy(\"l_shipdate\").save(parquet_dir)\n",
    "    print(f\"{sf_name} ({size_gb} Go) sauvegardé en Parquet à {parquet_dir}\")\n",
    "\n",
    "    sampled_df.write.mode(\"overwrite\").format(\"orc\").partitionBy(\"l_shipdate\").save(orc_dir)\n",
    "    print(f\"{sf_name} ({size_gb} Go) sauvegardé en ORC à {orc_dir}\")\n",
    "\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS local.db.lineitem_{sf_name}\")\n",
    "    sampled_df.write.mode(\"overwrite\").format(\"iceberg\").partitionBy(\"l_shipdate\").saveAsTable(f\"local.db.lineitem_{sf_name}\")\n",
    "    print(f\"{sf_name} ({size_gb} Go) sauvegardé en Iceberg à {iceberg_dir}\")\n",
    "\n",
    "    sampled_df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"l_shipdate\").save(delta_dir)\n",
    "    print(f\"{sf_name} ({size_gb} Go) sauvegardé en Delta Lake à {delta_dir}\")\n",
    "\n",
    "df.unpersist()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: option 'a' unknown.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dbgen: invalid option -- 'o'\n",
      "TPC-H Population Generator (Version 2.14.0 build 0)\n",
      "Copyright Transaction Processing Performance Council 1994 - 2010\n",
      "USAGE:\n",
      "dbgen [-{vf}][-T {pcsoPSOL}]\n",
      "\t[-s <scale>][-C <procs>][-S <step>]\n",
      "dbgen [-v] [-O m] [-s <scale>] [-U <updates>]\n",
      "\n",
      "Basic Options\n",
      "===========================\n",
      "-C <n> -- separate data set into <n> chunks (requires -S, default: 1)\n",
      "-f     -- force. Overwrite existing files\n",
      "-h     -- display this message\n",
      "-q     -- enable QUIET mode\n",
      "-s <n> -- set Scale Factor (SF) to  <n> (default: 1) \n",
      "-S <n> -- build the <n>th step of the data/update set (used with -C or -U)\n",
      "-U <n> -- generate <n> update sets\n",
      "-v     -- enable VERBOSE mode\n",
      "\n",
      "Advanced Options\n",
      "===========================\n",
      "-b <s> -- load distributions for <s> (default: dists.dss)\n",
      "-d <n> -- split deletes between <n> files (requires -U)\n",
      "-i <n> -- split inserts between <n> files (requires -U)\n",
      "-T c   -- generate cutomers ONLY\n",
      "-T l   -- generate nation/region ONLY\n",
      "-T L   -- generate lineitem ONLY\n",
      "-T n   -- generate nation ONLY\n",
      "-T o   -- generate orders/lineitem ONLY\n",
      "-T O   -- generate orders ONLY\n",
      "-T p   -- generate parts/partsupp ONLY\n",
      "-T P   -- generate parts ONLY\n",
      "-T r   -- generate region ONLY\n",
      "-T s   -- generate suppliers ONLY\n",
      "-T S   -- generate partsupp ONLY\n",
      "\n",
      "To generate the SF=1 (1GB), validation database population, use:\n",
      "\tdbgen -vf -s 1\n",
      "\n",
      "To generate updates for a SF=1 (1GB), use:\n",
      "\tdbgen -v -U 1 -s 1\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'dbgen -s 10 -f -o /app/data/tpch_raw/sf10' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Générer SF10 (10 Go)\u001b[39;00m\n\u001b[32m      8\u001b[39m sf10_dir = \u001b[33m\"\u001b[39m\u001b[33m/app/data/tpch_raw/sf10\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mgenerate_tpch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msf10_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mgenerate_tpch_data\u001b[39m\u001b[34m(scale_factor, output_dir)\u001b[39m\n\u001b[32m      2\u001b[39m os.makedirs(output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m cmd = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdbgen -s \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -f -o \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Toutes les tables\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDonnées TPC-H générées pour SF\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dans \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command 'dbgen -s 10 -f -o /app/data/tpch_raw/sf10' returned non-zero exit status 1."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
